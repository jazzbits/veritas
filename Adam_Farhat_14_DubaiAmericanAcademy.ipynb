{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM5J3vyx2Qkd5twJM2Pmg7I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":12,"metadata":{"id":"AdWDzrvLelCc","executionInfo":{"status":"ok","timestamp":1705670249955,"user_tz":-240,"elapsed":334,"user":{"displayName":"Adam Farhat","userId":"11870444074865108018"}}},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.impute import SimpleImputer\n","import seaborn as sns\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n","from math import sqrt"]},{"cell_type":"markdown","source":["## **1- Load the datasets**"],"metadata":{"id":"jVwdAm5y3iCQ"}},{"cell_type":"code","source":["# I will keep the datasets seperate so i wont risk the leakage of the lables into the\n","# data and eventuly I will have to split them to train the sci-kit model\n","\n","X_test_url = 'https://raw.githubusercontent.com/jazzbits/veritas/main/X_test.csv'\n","X_train_url = 'https://raw.githubusercontent.com/jazzbits/veritas/main/X_train.csv'\n","y_train_url = 'https://raw.githubusercontent.com/jazzbits/veritas/main/y_train.csv'\n","\n","X_test = pd.read_csv(X_test_url, index_col=0)\n","X_train = pd.read_csv(X_train_url, index_col=0)\n","y_train = pd.read_csv(y_train_url, index_col=0)"],"metadata":{"id":"i30s-uWk1xcS","executionInfo":{"status":"ok","timestamp":1705670250530,"user_tz":-240,"elapsed":576,"user":{"displayName":"Adam Farhat","userId":"11870444074865108018"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["## **2- Data Exploration/Preprocessing**"],"metadata":{"id":"M0tNU8ZK4w_Z"}},{"cell_type":"code","source":["# Drop the Application.Deadline column. We can extract relevany information\n","# for example month, day and year but It does not hold predictive values\n","X_train = X_train.drop('Application.Deadline', axis=1)\n","\n","# looks like data at index 119 is inconsistent so I will remove it\n","X_train = X_train.drop([119])\n","y_train = y_train.drop([119])\n","\n","# It also looks like the Column 'Earn' has non numeric data. the errors='coerce'\n","# option will replace any non numeric data with 'nan'\n","X_train['Earn'] = pd.to_numeric(X_train['Earn'], errors='coerce')\n"],"metadata":{"id":"qhAU5K69445X","executionInfo":{"status":"ok","timestamp":1705670250530,"user_tz":-240,"elapsed":3,"user":{"displayName":"Adam Farhat","userId":"11870444074865108018"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["## **3. Outlier Detection**"],"metadata":{"id":"4PrcswEGC0TQ"}},{"cell_type":"code","source":["numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n","categorical_cols = ['Ownership', 'Citytype']\n","\n","\n","# for col in numerical_cols:\n","#     Q1 = X_train[col].quantile(0.25)\n","#     Q3 = X_train[col].quantile(0.75)\n","#     IQR = Q3 - Q1\n","\n","#     lower_bound = Q1 - 1.5 * IQR\n","#     upper_bound = Q3 + 1.5 * IQR\n","\n","#     outliers = X_train[(X_train[col] < lower_bound) | (X_train[col] > upper_bound)]\n","\n","#     if not outliers.empty:\n","#         plt.figure(figsize=(10, 6))\n","#         plt.boxplot(X_train[col].dropna())\n","#         plt.title(f'Box Plot of {col}')\n","#         plt.ylabel(col)\n","#         plt.show()\n","\n","\n","# The following features have few outliers\n","# 'CrimeRate', 'FBI.CrimeRate', 'FBI.TotalCrime' has only few but\n","# 'Earn','Enrollment' there are lots of outliers.\n","\n","# I will not remove the 'outliers' becasue it could be a variability or actual data especially\n","# we are dealing with Earnings and Enrollment. for example and Earnign of 120k is reasonable\n","# and cannot be removed from the data\n","\n","# Also the 'Majors' features is supposed to be binary but i found instances of the number 2\n","# I will keep it as it might indicate a dual major"],"metadata":{"id":"S4w7eI7yC6CQ","executionInfo":{"status":"ok","timestamp":1705670250530,"user_tz":-240,"elapsed":3,"user":{"displayName":"Adam Farhat","userId":"11870444074865108018"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["## **4. Impute missing values**"],"metadata":{"id":"b1HI5bMfEcKc"}},{"cell_type":"code","source":["# Iam trying to get the count of the missing values in each column\n","nan_counts = X_train.isna().sum()\n","\n","# Look like SAT and ACT are missing a lot of and almost everyone\n","# who missed the SAT missed the ACT.\n","\n","\n","columns_to_impute = ['SAT', 'ACT', 'Earn', 'AvgCost', 'ADMrate']\n","for col in columns_to_impute:\n","    X_train[col] = SimpleImputer(strategy='mean').fit_transform(X_train[[col]])\n","\n","# correlation_matrix = X_train[numerical_cols].corr()\n","# plt.figure(figsize=(10, 8))\n","# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)\n","# plt.title(\"Correlation Matrix\")\n","# plt.show()\n","\n","# ACT and SAT are highly corrolated even after nan imputation.\n","# I will drop the ACT since it has more missing values\n","X_train = X_train.drop('ACT', axis=1)\n","\n"],"metadata":{"id":"BGI7rDZWEgKD","executionInfo":{"status":"ok","timestamp":1705670250530,"user_tz":-240,"elapsed":3,"user":{"displayName":"Adam Farhat","userId":"11870444074865108018"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## **5. One hot enconding**"],"metadata":{"id":"iPZ3A1Z1HgmI"}},{"cell_type":"code","source":["# The dataset contains categorical columns\n","X_train = pd.get_dummies(X_train, columns=categorical_cols)"],"metadata":{"id":"-ct-_xAMHvFW","executionInfo":{"status":"ok","timestamp":1705670250530,"user_tz":-240,"elapsed":3,"user":{"displayName":"Adam Farhat","userId":"11870444074865108018"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["## **6. Model selection and fine tuning:**"],"metadata":{"id":"zk2c6ivuJSvg"}},{"cell_type":"code","source":["# I will use  Random forest\n","model = RandomForestRegressor(random_state=42)\n","\n","# I will use gridsearch in order to fine tune the hyperparameters for the Regressor\n","# and I will combine it with cross validation so i can evaluate the performace of the model\n","\n","# 1- Define the hyperparameter values for the grid\n","grid_param = {\n","                'n_estimators': [100, 200, 300],\n","                'max_depth': [None, 10, 20, 30],\n","                'min_samples_split': [2, 5, 10],\n","                'min_samples_leaf': [1, 2, 4]\n","              }\n","\n","# 2- Configure the grid search\n","grid_search = GridSearchCV( estimator = model,  # RandomForest\n","\t\t\t                      param_grid = grid_param,  # this is the grid of parameters I defined above\n","                            cv = 3, #Cross validation with 3 folds\n","\t\t\t                      n_jobs = -1, # use all CPU cores\n","                            verbose = 0, # how much information gets printed out - 0 = moderate\n","                            scoring = 'neg_mean_squared_error' # Method used for evelualting the model performace\n","                           )\n","\n","# 3- computation\n","grid_search.fit(X_train, y_train.values.ravel())\n","\n","# 4- print the best parameters\n","print(\"The Best parameters from grid search:\", grid_search.best_params_)\n","\n","# 5-  I will use the model fine tuned with the best parameters from grid search\n","best_model = grid_search.best_estimator_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ks-tFVTHJh9A","executionInfo":{"status":"ok","timestamp":1705670367395,"user_tz":-240,"elapsed":116867,"user":{"displayName":"Adam Farhat","userId":"11870444074865108018"}},"outputId":"8205a5db-a6d9-4a9b-dc27-67eb825a9870"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["The Best parameters from grid search: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n"]}]},{"cell_type":"markdown","source":["## **7. Model Performace Evaluation**\n","\n","\n"],"metadata":{"id":"vsZdquOCNFcA"}},{"cell_type":"code","source":["# 1- Get the model predicted values on the traing dataset\n","predictions = best_model.predict(X_train)\n","\n","# 2- Get the mean squared error\n","mean_sq_err = mean_squared_error(y_train, predictions)\n","print(\"Mean Squared Error of the best model:\", mean_sq_err)\n","\n","# 3- Get the root mean squared error\n","root_mean_sq_err = sqrt(mean_sq_err)\n","print(\"       Root Mean Squared Error Score:\", root_mean_sq_err)\n","\n","# 4- Get the R² Score\n","r_square = r2_score(y_train, predictions)\n","print(\"                        The R² Score:\", r_square)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jPrZmUWGNM3g","executionInfo":{"status":"ok","timestamp":1705670367395,"user_tz":-240,"elapsed":5,"user":{"displayName":"Adam Farhat","userId":"11870444074865108018"}},"outputId":"0539bf28-aaef-4c89-ada4-7a68b3ad6428"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Squared Error of the best model: 4.066657940945877\n","       Root Mean Squared Error Score: 2.0165956314903286\n","                        The R² Score: 0.9028603927248733\n"]}]},{"cell_type":"markdown","source":["## **8. Start prediction on the test dataset**"],"metadata":{"id":"VIKWviVIdPWQ"}},{"cell_type":"code","source":["X_test = X_test.drop('Application.Deadline', axis=1)\n","X_test = X_test.drop('ACT', axis=1)\n","columns_to_impute = ['SAT', 'Earn', 'AvgCost', 'ADMrate','Enrollment']\n","\n","for col in columns_to_impute:\n","    X_test[col] = SimpleImputer(strategy='mean').fit_transform(X_test[[col]])\n","\n","X_test = pd.get_dummies(X_test, columns=categorical_cols)\n","X_test = X_test.reindex(columns=X_train.columns, fill_value=0)"],"metadata":{"id":"W0_HY82qhTFb","executionInfo":{"status":"ok","timestamp":1705670367395,"user_tz":-240,"elapsed":4,"user":{"displayName":"Adam Farhat","userId":"11870444074865108018"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["test_predictions = best_model.predict(X_test)\n","predictions_df = pd.DataFrame(test_predictions, index=X_test.index, columns=['prediction'])\n","print(predictions_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KtxXIsses1LT","executionInfo":{"status":"ok","timestamp":1705670367395,"user_tz":-240,"elapsed":4,"user":{"displayName":"Adam Farhat","userId":"11870444074865108018"}},"outputId":"c67546b2-8f8b-44b7-8ac7-be43d7e0fd6a"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["     prediction\n","14    52.528588\n","109   56.933520\n","159   44.331564\n","162   56.310250\n","253   52.385339\n","..          ...\n","98    48.006835\n","142   52.434833\n","62    54.330706\n","203   59.432126\n","33    52.558417\n","\n","[90 rows x 1 columns]\n"]}]},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","# path = '/content/drive/My Drive/Colab Notebooks/Veritas/Adam_Farhat_14_DubaiAmericanAcademy.csv'\n","# predictions_df.to_csv(path, index=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NYwP--fJ0tWI","executionInfo":{"status":"ok","timestamp":1705670370169,"user_tz":-240,"elapsed":2777,"user":{"displayName":"Adam Farhat","userId":"11870444074865108018"}},"outputId":"33d6328e-cb5f-454d-95ac-3ab3b0c3de5b"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]}]}